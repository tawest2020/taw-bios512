{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.3.3"},"colab":{"provenance":[]}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"77b97eda-a875-49d9-922a-aca98f18d1ea","cell_type":"markdown","source":"# Homework 08","metadata":{"id":"77b97eda-a875-49d9-922a-aca98f18d1ea"}},{"id":"ffcb226d-19da-439c-9729-1321f5ef0109","cell_type":"markdown","source":"### https://github.com/tawest2020/taw-bios512","metadata":{}},{"id":"tjtQS2sCOzfG","cell_type":"code","source":"library(tidyverse)","metadata":{"id":"tjtQS2sCOzfG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"45f8f303-c7d1-4dbf-9dc1-f33c108bcf63","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.2     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.1     \n── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"}],"execution_count":1},{"id":"f75ac688-c3c4-4916-a858-97104be7e273","cell_type":"markdown","source":"## Question 1","metadata":{"id":"f75ac688-c3c4-4916-a858-97104be7e273"}},{"id":"9eb3eefd-fd8c-4688-8b9e-e2368e56d526","cell_type":"markdown","source":"#### a) What are the steps of `kmeans`?","metadata":{"id":"9eb3eefd-fd8c-4688-8b9e-e2368e56d526"}},{"id":"5n9KJZJG1WgF","cell_type":"markdown","source":"1. Assign each point to a cluster N at random.  \n2. Calculate the mean position of each cluster using the previous assignments.  \n3. Loop through the points - assign each point to the cluster to whose center it is closest.  \n4. Repeat this process until the centers stop moving around.","metadata":{"id":"5n9KJZJG1WgF"}},{"id":"bf8b199b-d1d9-4a55-ad95-9e86ad0655bd","cell_type":"markdown","source":"#### b) Create the builder function for step 1.","metadata":{"id":"bf8b199b-d1d9-4a55-ad95-9e86ad0655bd"}},{"id":"AdQ4OobYN017","cell_type":"code","source":"builder <- function(n_points, n_clusters){\n  sample(((1:n_points) %% n_clusters)+1, n_points, replace=F)\n}","metadata":{"id":"AdQ4OobYN017","trusted":true},"outputs":[],"execution_count":2},{"id":"9e44ddaf-4506-47d0-98fb-09871e08808c","cell_type":"markdown","source":"#### c) Create the builder function for step 2.","metadata":{"id":"9e44ddaf-4506-47d0-98fb-09871e08808c"}},{"id":"YQSBESAdN5Pz","cell_type":"code","source":"get_cluster_means <- function(data, labels){\n  data %>%\n    mutate(label__ = labels) %>%\n    group_by(label__) %>%\n    summarize(across(everything(), mean), .groups = \"drop\") %>%\n    arrange(label__)\n}","metadata":{"id":"YQSBESAdN5Pz","trusted":true},"outputs":[],"execution_count":3},{"id":"1c8640c6-2d2f-49c3-b7ed-da0d4fb13935","cell_type":"markdown","source":"#### d) Create the builder function for step 3. ","metadata":{"id":"1c8640c6-2d2f-49c3-b7ed-da0d4fb13935"}},{"id":"L7RpgG4SN-CO","cell_type":"code","source":"assign_cluster_fast <- function(data, means){\n  data_matrix <- as.matrix(data)\n  means_matrix <- as.matrix(means %>% dplyr::select(-label__))\n  dii <- sort(rep(1:nrow(data), nrow(means)))\n  mii <- rep(1:nrow(means), nrow(data))\n  data_repped <- data_matrix[dii, ]\n  means_repped <- means_matrix[mii, ]\n  diff_squared <- (data_repped - means_repped)^2\n  all_distances <- rowSums(diff_squared)\n  tibble(dii=dii, mii=mii, distance=all_distances) %>%\n    group_by(dii) %>%\n    arrange(distance) %>%\n    filter(row_number()==1) %>%\n    ungroup() %>%\n    arrange(dii) %>%\n    pull(mii)\n}","metadata":{"id":"L7RpgG4SN-CO","trusted":true},"outputs":[],"execution_count":4},{"id":"2bbea660-9bd2-4dae-9a5c-838f613b0d7c","cell_type":"markdown","source":"#### e) Create the builder function for step 4.","metadata":{"id":"2bbea660-9bd2-4dae-9a5c-838f613b0d7c"}},{"id":"PuLctamsOCmG","cell_type":"code","source":"kmeans_done <- function(old_means, new_means, eps=1e-6){\n  om <- as.matrix(old_means)\n  nm <- as.matrix(new_means)\n  m <- mean(sqrt(rowSums((om - nm)^2)))\n  if(m < eps) TRUE else FALSE\n}\n\nmykmeans <- function(data, n_clusters, eps=1e-6, max_it = 1000, verbose = FALSE){\n  labels <- builder(nrow(data), n_clusters)\n  old_means <- get_cluster_means(data, labels)\n  done <- FALSE\n  it <- 0\n  while(!done & it < max_it){\n    labels <- assign_cluster_fast(data, old_means)\n    new_means <- get_cluster_means(data, labels)\n    if(kmeans_done(old_means, new_means)){\n      done <- TRUE\n    } else {\n      old_means <- new_means\n      it <- it + 1\n      if(verbose){\n        cat(sprintf(\"%d\\n\", it))\n      }\n    }\n  }\n  list(labels=labels, means=new_means)\n}","metadata":{"id":"PuLctamsOCmG","trusted":true},"outputs":[],"execution_count":5},{"id":"f3c86f61-68d2-484a-9c9c-db7f35c8e685","cell_type":"markdown","source":"#### f) Combine them all into your own `kmeans` function.","metadata":{"id":"f3c86f61-68d2-484a-9c9c-db7f35c8e685"}},{"id":"d33df395-de62-4cd3-a501-bac755a4549b","cell_type":"code","source":"label_randomly <- function(n_points, n_clusters){\n  sample(((1:n_points) %% n_clusters)+1, n_points, replace=F)\n}\n\nget_cluster_means <- function(data, labels){\n  data %>%\n    mutate(label__ = labels) %>%\n    group_by(label__) %>%\n    summarize(across(everything(), mean), .groups = \"drop\") %>%\n    arrange(label__)\n}\n\nassign_cluster_fast <- function(data, means){\n  data_matrix <- as.matrix(data)\n  means_matrix <- as.matrix(means %>% dplyr::select(-label__))\n  dii <- sort(rep(1:nrow(data), nrow(means)))\n  mii <- rep(1:nrow(means), nrow(data))\n  data_repped <- data_matrix[dii, ]\n  means_repped <- means_matrix[mii, ]\n  diff_squared <- (data_repped - means_repped)^2\n  all_distances <- rowSums(diff_squared)\n  tibble(dii=dii, mii=mii, distance=all_distances) %>%\n    group_by(dii) %>%\n    arrange(distance) %>%\n    filter(row_number()==1) %>%\n    ungroup() %>%\n    arrange(dii) %>%\n    pull(mii)\n}\n\nkmeans_done <- function(old_means, new_means, eps=1e-6){\n  om <- as.matrix(old_means)\n  nm <- as.matrix(new_means)\n  m <- mean(sqrt(rowSums((om - nm)^2)))\n  if(m < eps) TRUE else FALSE\n}\n\nmykmeans <- function(data, n_clusters, eps=1e-6, max_it = 1000, verbose = FALSE){\n  labels <- label_randomly(nrow(data), n_clusters)\n  old_means <- get_cluster_means(data, labels)\n  done <- FALSE\n  it <- 0\n  while(!done & it < max_it){\n    labels <- assign_cluster_fast(data, old_means)\n    new_means <- get_cluster_means(data, labels)\n    if(kmeans_done(old_means, new_means)){\n      done <- TRUE\n    } else {\n      old_means <- new_means\n      it <- it + 1\n      if(verbose){\n        cat(sprintf(\"%d\\n\", it))\n      }\n    }\n  }\n  list(labels=labels, means=new_means)\n}","metadata":{"vscode":{"languageId":"r"},"id":"d33df395-de62-4cd3-a501-bac755a4549b","trusted":false},"outputs":[],"execution_count":9},{"id":"da13180d-3a51-4218-94a0-3f362612f099","cell_type":"markdown","source":"## Question 2\nThis is when we'll test your `kmeans` function.\n#### a) Read in the `voltages_df.csv` data set.","metadata":{"id":"da13180d-3a51-4218-94a0-3f362612f099"}},{"id":"niZbE104OWt_","cell_type":"code","source":"vdf <- read_csv(\"voltages_df.csv\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niZbE104OWt_","outputId":"fa767157-1700-47c3-8ac6-4cd976ae6eef","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"\u001b[1mRows: \u001b[22m\u001b[34m900\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m250\u001b[39m\n\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n\u001b[1mDelimiter:\u001b[22m \",\"\n\u001b[32mdbl\u001b[39m (250): 0, 1.00401606425703, 2.00803212851406, 3.01204819277108, 4.016064...\n\n\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"}],"execution_count":6},{"id":"1f002a7b-eda1-4d2d-8ef9-3090b563708d","cell_type":"markdown","source":"#### b) Call your `kmeans` function with 3 clusters. Print the results with `results$labels` and `results$means`.","metadata":{"id":"1f002a7b-eda1-4d2d-8ef9-3090b563708d"}},{"id":"eJhiOgx-On3J","cell_type":"code","source":"myresults <- mykmeans(vdf, 3)\n\nprint(myresults$labels)\nprint(myresults$means)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJhiOgx-On3J","outputId":"f95a4ec4-e143-4e5b-8cfe-a332e8c56962","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"  [1] 2 3 3 3 3 3 1 1 3 2 2 2 1 2 3 2 2 2 2 2 3 3 3 3 3 2 2 2 1 1 1 1 3 1 1 3 3\n [38] 2 1 3 2 2 2 1 3 1 1 3 1 1 3 1 2 2 1 2 3 2 1 3 2 2 3 2 1 1 3 1 2 1 2 3 1 1\n [75] 3 3 2 1 1 1 1 1 3 1 1 3 3 3 2 1 1 1 3 1 3 3 1 3 3 1 1 1 3 1 2 2 1 2 2 3 2\n[112] 2 3 1 2 1 2 2 3 3 3 3 2 2 1 3 3 1 2 1 3 2 3 1 2 3 3 3 2 1 2 3 3 1 2 2 3 2\n[149] 2 1 2 1 3 3 1 2 1 1 3 2 1 1 2 3 2 1 3 2 3 2 2 1 3 2 1 1 3 1 2 3 1 3 1 2 1\n[186] 3 3 2 3 2 2 3 2 2 1 1 2 3 2 2 1 2 1 1 3 2 3 1 3 2 3 1 2 1 1 3 1 1 3 3 3 3\n[223] 3 3 3 1 1 1 1 2 3 2 1 3 3 1 1 2 2 1 1 2 1 1 3 1 2 1 1 3 1 3 2 1 3 1 1 2 2\n[260] 2 3 3 1 2 3 3 3 1 1 1 2 3 3 3 1 2 1 1 3 3 1 2 1 2 2 3 1 3 3 1 1 2 1 1 2 3\n[297] 2 1 1 3 3 1 2 2 2 1 2 3 3 3 2 2 2 2 2 2 3 2 3 2 2 1 3 1 2 2 2 2 3 2 3 2 1\n[334] 2 3 3 1 1 2 3 1 3 1 1 2 1 3 1 2 2 1 3 3 3 2 1 3 2 1 1 3 3 3 3 2 1 2 3 2 2\n[371] 2 2 2 3 3 1 1 2 2 2 2 1 1 2 2 1 1 3 3 1 2 3 3 3 3 1 3 3 1 2 2 2 2 3 3 2 2\n[408] 1 1 1 3 1 1 2 3 3 3 1 3 3 3 2 2 3 2 1 3 3 3 3 1 1 3 1 2 3 2 2 2 1 1 1 1 2\n[445] 3 2 2 2 1 3 2 2 2 3 3 2 3 3 2 1 3 3 2 1 3 3 3 2 1 1 2 1 2 3 3 1 2 1 2 3 3\n[482] 1 2 1 1 1 3 1 1 1 1 1 3 2 2 1 1 2 3 3 3 2 2 2 3 3 1 1 2 1 2 3 3 1 1 2 1 1\n[519] 3 2 2 2 3 1 2 3 1 1 2 3 1 1 1 1 2 2 3 2 2 3 3 3 3 1 2 2 1 2 2 2 3 3 2 3 1\n[556] 1 3 1 1 1 3 1 1 3 1 1 3 1 1 2 1 3 1 3 1 3 3 3 2 1 1 1 1 2 3 1 1 1 3 1 1 3\n[593] 3 3 1 2 3 3 2 3 1 3 3 1 2 2 1 1 1 3 1 2 3 2 1 3 2 3 3 2 1 3 2 1 1 2 3 2 1\n[630] 3 2 1 2 3 3 2 3 2 2 2 2 2 3 3 2 1 2 1 1 2 1 3 1 2 1 2 2 2 3 2 2 3 3 2 2 3\n[667] 2 3 1 1 3 2 3 2 2 1 3 3 2 1 2 3 1 1 2 2 3 2 3 3 1 1 1 1 3 3 3 2 1 3 2 1 1\n[704] 2 2 1 3 1 1 2 3 2 1 3 2 3 3 2 3 3 2 2 3 3 2 3 1 2 2 2 2 2 3 3 2 2 3 3 2 3\n[741] 2 3 1 2 2 2 3 2 1 3 3 2 2 1 2 2 1 2 1 1 2 2 3 1 2 3 1 3 3 2 1 3 1 2 3 2 2\n[778] 1 2 1 3 2 2 3 1 3 2 3 2 1 2 2 3 3 3 2 3 2 1 3 3 3 3 2 2 1 3 2 2 2 2 1 1 2\n[815] 1 2 3 2 3 1 3 3 3 3 3 1 1 3 1 1 1 1 1 1 3 2 2 3 1 1 2 3 1 1 1 1 2 2 3 1 3\n[852] 2 1 1 1 1 2 2 1 1 1 1 1 2 3 1 3 2 3 3 1 2 1 1 2 2 3 2 2 2 1 3 1 3 3 2 2 1\n[889] 1 3 2 1 1 3 3 3 3 2 1 1\n\u001b[90m# A tibble: 3 × 251\u001b[39m\n  label__   `0` `1.00401606425703` `2.00803212851406` `3.01204819277108`\n    \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n\u001b[90m1\u001b[39m       1 -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m3\u001b[39m              1.31               1.16               0.979\n\u001b[90m2\u001b[39m       2 -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m3\u001b[39m              0.938              0.762              0.363\n\u001b[90m3\u001b[39m       3 -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m3\u001b[39m              1.24               1.09               0.900\n\u001b[90m# ℹ 246 more variables: `4.01606425702811` <dbl>, `5.02008032128514` <dbl>,\u001b[39m\n\u001b[90m#   `6.02409638554217` <dbl>, `7.0281124497992` <dbl>,\u001b[39m\n\u001b[90m#   `8.03212851405623` <dbl>, `9.03614457831325` <dbl>,\u001b[39m\n\u001b[90m#   `10.0401606425703` <dbl>, `11.0441767068273` <dbl>,\u001b[39m\n\u001b[90m#   `12.0481927710843` <dbl>, `13.0522088353414` <dbl>,\u001b[39m\n\u001b[90m#   `14.0562248995984` <dbl>, `15.0602409638554` <dbl>,\u001b[39m\n\u001b[90m#   `16.0642570281125` <dbl>, `17.0682730923695` <dbl>, …\u001b[39m\n"}],"execution_count":7},{"id":"03d9949c-60ce-4a75-8688-e3c7485122ab","cell_type":"markdown","source":"#### c) Call R's `kmeans` function with 3 clusters. Print the results with `results$labels` and `results$cluster`.","metadata":{"id":"03d9949c-60ce-4a75-8688-e3c7485122ab"}},{"id":"Tbe8l-BCP5f_","cell_type":"code","source":"vdf_matrix <- as.matrix(vdf)\nresults <- kmeans(vdf_matrix, centers = 3)\n\nprint(results$centers)\nprint(results$cluster)","metadata":{"id":"Tbe8l-BCP5f_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c56f65c0-47d9-4233-9813-7f8c79dc7e36","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"          0 1.00401606425703 2.00803212851406 3.01204819277108 4.01606425702811\n1 -1.031463        1.3093239        1.1616772        0.9787498        0.6481497\n2 -1.031463        1.2439759        1.0924697        0.9004440        0.3011754\n3 -1.031463        0.9381238        0.7619864        0.3631543       -1.1179412\n  5.02008032128514 6.02409638554217 7.0281124497992 8.03212851405623\n1        -1.168610       -1.1196122      -1.0590962       -0.9943176\n2        -1.159714       -1.1098127      -1.0685484       -1.0338649\n3        -1.051145       -0.9766807      -0.8694758       -0.6892375\n  9.03614457831325 10.0401606425703 11.0441767068273 12.0481927710843\n1       -0.9237437       -0.8457536       -0.7572129       -0.6201221\n2       -1.0022396       -0.9699741       -0.9343697       -0.8943605\n3       -0.5661321       -0.2497152        0.6027358        0.6960355\n  13.0522088353414 14.0562248995984 15.0602409638554 16.0642570281125\n1       -0.1849969        0.5921819        0.6827391        0.1509102\n2       -0.8507035       -0.8060072       -0.7647395       -0.7329406\n3        0.3917273       -0.9473202       -1.0883844       -1.0443762\n  17.0682730923695 18.0722891566265 19.0763052208835 20.0803212851406\n1       -0.8453464       -1.0161305       -0.9911278       -0.9534526\n2       -0.7162848       -0.7155110       -0.7229214       -0.7325077\n3       -1.0057512       -0.9698542       -0.9344880       -0.8979255\n  21.0843373493976 22.0883534136546 23.0923694779116 24.0963855421687\n1       -0.9278338       -0.9128185       -0.9036753       -0.8938970\n2       -0.7420098       -0.7501943       -0.7564416       -0.7604952\n3       -0.8583498       -0.8130091       -0.7569233       -0.6702728\n  25.1004016064257 26.1044176706827 27.1084337349398 28.1124497991968\n1       -0.8772257       -0.8486353       -0.8039844       -0.7376698\n2       -0.7624458       -0.7628025       -0.7623710       -0.7617723\n3       -0.3675540        0.5506211        0.9011507        0.6795731\n  29.1164658634538 30.1204819277108 31.1244979919679 32.1285140562249\n1       -0.6036243       0.06859829        0.8128548        0.8645315\n2       -0.7607406      -0.75765775       -0.7496843       -0.7333197\n3       -0.3524483      -1.04171502       -1.0459201       -0.9892448\n  33.1325301204819 34.136546184739 35.140562248996 36.144578313253\n1        0.3007530      -0.8509538      -1.0593609     -0.99733097\n2       -0.7046809      -0.6531653      -0.4914149     -0.01286284\n3       -0.9443322      -0.9096994      -0.8815063     -0.85520281\n  37.1485943775101 38.1526104417671 39.1566265060241 40.1606425702811\n1       -0.9264019       -0.8615397      -0.80559654       -0.7625137\n2        0.4111138        0.4708061      -0.08318854       -0.6569424\n3       -0.8276580       -0.7988569      -0.77307667       -0.7587053\n  41.1646586345382 42.1686746987952 43.1726907630522 44.1767068273092\n1       -0.7331843       -0.7265177       -0.7332993       -0.7452244\n2       -0.9102454       -0.8994279       -0.7365828       -0.6090580\n3       -0.7632138       -0.7847630       -0.8145071       -0.8440694\n  45.1807228915663 46.1847389558233 47.1887550200803 48.1927710843374\n1       -0.7541076       -0.7630892       -0.7738323       -0.7890510\n2       -0.4429131       -0.1988909        0.3264548        0.4558868\n3       -0.8675246       -0.8809781       -0.8820439       -0.8695599\n  49.1967871485944 50.2008032128514 51.2048192771084 52.2088353413655\n1       -0.8105414       -0.8376561       -0.8674783       -0.8961943\n2        0.1505446       -0.5861572       -0.9231358       -1.0150048\n3       -0.8435323       -0.8053154       -0.7581727       -0.7076670\n  53.2128514056225 54.2168674698795 55.2208835341365 56.2248995983936\n1       -0.9203792       -0.9376461       -0.9467293       -0.9471597\n2       -0.9812155       -0.9519898       -0.9255601       -0.8963167\n3       -0.6518253       -0.6418336       -0.6683513       -0.7186505\n  57.2289156626506 58.2329317269076 59.2369477911647 60.2409638554217\n1       -0.9387284       -0.9209543       -0.8927547       -0.8524303\n2       -0.8594356       -0.8113235       -0.7465117       -0.6458075\n3       -0.7510866       -0.7687534       -0.7789676       -0.7875447\n  61.2449799196787 62.2489959839357 63.2530120481928 64.2570281124498\n1       -0.7977063       -0.7247251       -0.5933942      -0.02666874\n2       -0.4698973       -0.2004182       -0.2769707      -0.58780082\n3       -0.7974017       -0.8105436       -0.8270493      -0.84511879\n  65.2610441767068 66.2650602409639 67.2690763052209 68.2730923694779\n1        0.4729405        0.4144252       -0.4315513       -1.0008879\n2       -0.8843039       -0.9061849       -0.9151609       -0.9136858\n3       -0.8619288       -0.8745335       -0.8804324       -0.8778206\n  69.2771084337349 70.281124497992 71.285140562249 72.289156626506\n1       -1.0090643      -0.9817530      -0.9556980      -0.9281157\n2       -0.8979656      -0.8643462      -0.8085071      -0.7200554\n3       -0.8656564      -0.8436347      -0.8120791      -0.7717843\n  73.2931726907631 74.2971887550201 75.3012048192771 76.3052208835341\n1       -0.8958956       -0.8562554       -0.8077624       -0.7520997\n2       -0.3865882        0.5166925        0.8451196        0.6232395\n3       -0.7224965       -0.6589461       -0.5355189       -0.4204487\n  77.3092369477912 78.3132530120482 79.3172690763052 80.3212851405623\n1       -0.6983205       -0.6636690       -0.6749499       -0.7301447\n2       -0.5961787       -1.0623477       -1.0724163       -1.0356110\n3       -0.4492026       -0.6247444       -0.7674160       -0.8087844\n  81.3253012048193 82.3293172690763 83.3333333333333 84.3373493975904\n1       -0.8166818       -0.8587393       -0.8799611       -0.8807739\n2       -0.9954282       -0.9528213       -0.9064278       -0.8582115\n3       -0.8300087       -0.8472407       -0.8581972       -0.8615963\n  85.3413654618474 86.3453815261044 87.3493975903614 88.3534136546185\n1       -0.8565428       -0.8006910       -0.6918535       0.01211907\n2       -0.8140585       -0.7829844       -0.7718335      -0.77702656\n3       -0.8575195       -0.8475783       -0.8346705      -0.82209589\n  89.3574297188755 90.3614457831325 91.3654618473896 92.3694779116466\n1        1.0445785        0.9417920        0.7389057       -0.7150875\n2       -0.7871946       -0.7928432       -0.7890063       -0.7742481\n3       -0.8120214       -0.8040541       -0.7951620       -0.7810723\n  93.3734939759036 94.3775100401606 95.3815261044177 96.3855421686747\n1       -1.1089291       -1.0556228       -1.0180026       -0.9953800\n2       -0.7499086       -0.7199963       -0.6912508       -0.6620338\n3       -0.7579365       -0.7232338       -0.6641211       -0.5714035\n  97.3895582329317 98.3935742971888 99.3975903614458 100.401606425703\n1       -0.9809828       -0.9669849        -0.947114       -0.9178894\n2       -0.6573335       -0.6680024        -0.693500       -0.6899649\n3       -0.4183154       -0.3719077        -0.503291       -0.7008495\n  101.40562248996 102.409638554217 103.413654618474 104.417670682731\n1      -0.8791665       -0.8349946       -0.7952200       -0.7756150\n2      -0.6756290       -0.6511730       -0.5867081       -0.4940197\n3      -0.7746358       -0.7625038       -0.7285317       -0.6571094\n  105.421686746988 106.425702811245 107.429718875502 108.433734939759\n1       -0.7869612       -0.8196682       -0.8572411       -0.8888497\n2       -0.3959773       -0.4019194       -0.4836075       -0.6031085\n3       -0.4043544        0.1265605        0.3638254        0.1859342\n  109.437751004016 110.441767068273 111.44578313253 112.449799196787\n1       -0.9084385       -0.9129518      -0.9016455       -0.8760331\n2       -0.6803393       -0.7009213      -0.7220232       -0.7586346\n3       -0.4285780       -0.7651785      -0.9118234       -0.9492127\n  113.453815261044 114.457831325301 115.461847389558 116.465863453815\n1       -0.8402975       -0.8021679       -0.7735978       -0.7672098\n2       -0.7931128       -0.8129309       -0.8240816       -0.8223207\n3       -0.9263633       -0.9037028       -0.8843812       -0.8642445\n  117.469879518072 118.473895582329 119.477911646586 120.481927710843\n1       -0.7838746       -0.8118719       -0.8402008       -0.8622569\n2       -0.8048594       -0.7706219       -0.7204532       -0.6522148\n3       -0.8387891       -0.8041192       -0.7572864       -0.6936271\n  121.4859437751 122.489959839357 123.493975903614 124.497991967871\n1     -0.8743578       -0.8745548       -0.8621162      -0.83738609\n2     -0.4767159       -0.4081569       -0.5063971      -0.76474266\n3     -0.5608583       -0.1942616        0.1086731       0.02132084\n  125.502008032129 126.506024096386 127.510040160643 128.5140562249\n1       -0.8019045       -0.7588503       -0.7139745     -0.6607540\n2       -0.8404665       -0.8547187       -0.8574783     -0.8470736\n3       -0.5326843       -0.8758425       -0.9333571     -0.9340874\n  129.518072289157 130.522088353414 131.526104417671 132.530120481928\n1       -0.6276292       -0.6446482       -0.7145974       -0.7611321\n2       -0.8237992       -0.7907006       -0.7549813       -0.7297735\n3       -0.9361184       -0.9390643       -0.9391331       -0.9326577\n  133.534136546185 134.538152610442 135.542168674699 136.546184738956\n1       -0.7857777       -0.8012031       -0.8125440       -0.8201603\n2       -0.7298815       -0.7529255       -0.7848950       -0.8165587\n3       -0.9172195       -0.8925731       -0.8614265       -0.8300492\n  137.550200803213 138.55421686747 139.558232931727 140.562248995984\n1       -0.8247583      -0.8271200       -0.8278544       -0.8272444\n2       -0.8425382      -0.8596402       -0.8659917       -0.8606862\n3       -0.8077430      -0.8024823       -0.8136626       -0.8322854\n  141.566265060241 142.570281124498 143.574297188755 144.578313253012\n1       -0.8252812       -0.8218972       -0.8172887       -0.8121326\n2       -0.8437425       -0.8162919       -0.7810048       -0.7418578\n3       -0.8483442       -0.8544481       -0.8456450       -0.8186060\n  145.582329317269 146.586345381526 147.590361445783 148.59437751004\n1       -0.8075060       -0.8044394       -0.8032921     -0.80337598\n2       -0.7067689       -0.6901358       -0.7059361     -0.73487063\n3       -0.7708364       -0.6983153       -0.4995888      0.06111762\n  149.598393574297 150.602409638554 151.606425702811 152.610441767068\n1       -0.8031237      -0.80065690       -0.7943595       -0.7832322\n2       -0.7666850      -0.79030791       -0.8088526       -0.8202897\n3        0.2785784      -0.02233891       -0.8344614       -1.0044925\n  153.614457831325 154.618473895582 155.622489959839 156.626506024096\n1       -0.7670602       -0.7465418       -0.7235183       -0.7012304\n2       -0.8228722       -0.8145883       -0.7924833       -0.7512216\n3       -0.9947944       -0.9812626       -0.9611036       -0.9329663\n  157.630522088353 158.63453815261 159.638554216867 160.642570281125\n1       -0.6807904      -0.6728119       -0.6832253       -0.7054310\n2       -0.6757423      -0.3063965        0.8688481        0.9679233\n3       -0.8974015      -0.8567861       -0.8149502       -0.7765148\n  161.646586345382 162.650602409639 163.654618473896 164.658634538153\n1       -0.7309402      -0.75082774       -0.7686511       -0.7816786\n2        0.7772804      -0.07183464       -1.1062019       -1.0341477\n3       -0.7457931      -0.72540822       -0.7155786       -0.7153215\n  165.66265060241 166.666666666667 167.670682730924 168.674698795181\n1      -0.7870605       -0.7817687       -0.7624390       -0.7245206\n2      -0.9682607       -0.9201250       -0.8912380       -0.8785268\n3      -0.7238374       -0.7399730       -0.7613774       -0.7843319\n  169.678714859438 170.682730923695 171.686746987952 172.690763052209\n1       -0.6548498       -0.3204002        0.6097598        0.9159961\n2       -0.8757421       -0.8765711       -0.8763897       -0.8723257\n3       -0.8048796       -0.8195663       -0.8257305       -0.8215405\n  173.694779116466 174.698795180723 175.70281124498 176.706827309237\n1        0.6867779       -0.3890717      -1.0366319       -1.0173625\n2       -0.8627871       -0.8471194      -0.8255329       -0.7992863\n3       -0.8059275       -0.7785542      -0.7399101       -0.6905890\n  177.710843373494 178.714859437751 179.718875502008 180.722891566265\n1       -0.9442263       -0.8744008       -0.8208368       -0.7929042\n2       -0.7711209       -0.7458106       -0.7301849       -0.7295152\n3       -0.5947843       -0.4888176       -0.4608156       -0.5897696\n  181.726907630522 182.730923694779 183.734939759036 184.738955823293\n1       -0.7922503       -0.8086421       -0.8305061       -0.8505003\n2       -0.7412643       -0.7592950       -0.7779570       -0.7932478\n3       -0.7499744       -0.8130170       -0.8299556       -0.8409305\n  185.74297188755 186.746987951807 187.751004016064 188.755020080321\n1      -0.8644851       -0.8698348       -0.8643891       -0.8458421\n2      -0.8027234       -0.8053583       -0.8015959       -0.7935596\n3      -0.8452711       -0.8433730       -0.8365057       -0.8265279\n  189.759036144578 190.763052208835 191.767068273092 192.771084337349\n1       -0.8111387       -0.7548184       -0.6525374       -0.1001499\n2       -0.7852354       -0.7820492       -0.7889309       -0.8074163\n3       -0.8155633       -0.8057013       -0.7987310       -0.7958760\n  193.775100401606 194.779116465863 195.78313253012 196.787148594378\n1        0.9391002        0.9285660       0.6517322       -0.7358678\n2       -0.8348509       -0.8664015      -0.8970687       -0.9225360\n3       -0.7975586       -0.8033236      -0.8120201       -0.8221502\n  197.791164658635 198.795180722892 199.799196787149 200.803212851406\n1       -1.0895200       -1.0154215       -0.9519020       -0.9026096\n2       -0.9394094       -0.9452854       -0.9387656       -0.9194256\n3       -0.8321654       -0.8405697       -0.8458446       -0.8463236\n  201.807228915663 202.81124497992 203.815261044177 204.819277108434\n1       -0.8696398      -0.8523352       -0.8471801       -0.8494938\n2       -0.8877457      -0.8449963       -0.7930944       -0.7343536\n3       -0.8401411      -0.8253304       -0.8000397       -0.7627423\n  205.823293172691 206.827309236948 207.831325301205 208.835341365462\n1       -0.8551073       -0.8607785       -0.8638455       -0.8618848\n2       -0.6691135       -0.4838896       -0.2999128       -0.1804235\n3       -0.7118802       -0.6216330       -0.4156647       -0.0534988\n  209.839357429719 210.843373493976 211.847389558233 212.85140562249\n1      -0.85264306       -0.8342272       -0.8054928      -0.7666557\n2      -0.35598898       -0.5162496       -0.2364068       0.3284619\n3       0.02474902       -0.2568694       -0.7093632      -0.8470668\n  213.855421686747 214.859437751004 215.863453815261 216.867469879518\n1       -0.7200529       -0.6620238       -0.6140577       -0.6263134\n2        0.3395453        0.1645395       -0.2989640       -0.4018100\n3       -0.8120225       -0.7573180       -0.6723234       -0.6164225\n  217.871485943775 218.875502008032 219.879518072289 220.883534136546\n1       -0.6911566       -0.7582236       -0.7853809       -0.7957568\n2       -0.3585309       -0.3290437       -0.5004164       -0.4362260\n3       -0.5988228       -0.6411442       -0.6640895       -0.6739551\n  221.887550200803 222.89156626506 223.895582329317 224.899598393574\n1       -0.7917830      -0.7707133       -0.7296389       -0.6592842\n2       -0.3927307      -0.6306939       -0.9151246       -0.9574830\n3       -0.6798595      -0.6872401       -0.7007792       -0.7162648\n  225.903614457831 226.907630522088 227.911646586345 228.915662650602\n1       -0.3841880        0.3496670        0.6369398        0.3579089\n2       -0.9648791       -0.9703784       -0.9712805       -0.9661478\n3       -0.7369719       -0.7604956       -0.7832985       -0.8022765\n  229.919678714859 230.923694779116 231.927710843374 232.931726907631\n1       -0.6477129       -1.0293754       -1.0263127       -0.9953023\n2       -0.9552204       -0.9406667       -0.9264022       -0.9170391\n3       -0.8152619       -0.8211078       -0.8195989       -0.8113355\n  233.935742971888 234.939759036145 235.943775100402 236.947791164659\n1       -0.9687414       -0.9424557       -0.9113858       -0.8701538\n2       -0.9158466       -0.9227888       -0.9344310       -0.9457199\n3       -0.7976862       -0.7808822       -0.7642316       -0.7521010\n  237.951807228916 238.955823293173 239.95983935743 240.963855421687\n1       -0.8128519       -0.7279844      -0.4718942        0.3364266\n2       -0.9518850       -0.9493809      -0.9360238       -0.9107472\n3       -0.7487914       -0.7560044      -0.7713512       -0.7900387\n  241.967871485944 242.971887550201 243.975903614458 244.979919678715\n1        0.8337474        0.7125412       -0.2659209       -1.0409179\n2       -0.8732292       -0.8234477       -0.7607812       -0.6682618\n3       -0.8070676       -0.8182598       -0.8207339       -0.8132928\n  245.983935742972 246.987951807229 247.991967871486 248.995983935743\n1       -1.0587745      -1.01359887      -0.96467777       -0.9151047\n2       -0.3380864      -0.04693168       0.02820486       -0.4113500\n3       -0.7969549      -0.77567272      -0.75689256       -0.7496483\n         250\n1 -0.8610245\n2 -0.8115784\n3 -0.7570393\n  [1] 3 2 2 2 2 2 1 1 2 3 3 3 1 3 2 3 3 3 3 3 2 2 2 2 2 3 3 3 1 1 1 1 2 1 1 2 2\n [38] 3 1 2 3 3 3 1 2 1 1 2 1 1 2 1 3 3 1 3 2 3 1 2 3 3 2 3 1 1 2 1 3 1 3 2 1 1\n [75] 2 2 3 1 1 1 1 1 2 1 1 2 2 2 3 1 1 1 2 1 2 2 1 2 2 1 1 1 2 1 3 3 1 3 3 2 3\n[112] 3 2 1 3 1 3 3 2 2 2 2 3 3 1 2 2 1 3 1 2 3 2 1 3 2 2 2 3 1 3 2 2 1 3 3 2 3\n[149] 3 1 3 1 2 2 1 3 1 1 2 3 1 1 3 2 3 1 2 3 2 3 3 1 2 3 1 1 2 1 3 2 1 2 1 3 1\n[186] 2 2 3 2 3 3 2 3 3 1 1 3 2 3 3 1 3 1 1 2 3 2 1 2 3 2 1 3 1 1 2 1 1 2 2 2 2\n[223] 2 2 2 1 1 1 1 3 2 3 1 2 2 1 1 3 3 1 1 3 1 1 2 1 3 1 1 2 1 2 3 1 2 1 1 3 3\n[260] 3 2 2 1 3 2 2 2 1 1 1 3 2 2 2 1 3 1 1 2 2 1 3 1 3 3 2 1 2 2 1 1 3 1 1 3 2\n[297] 3 1 1 2 2 1 3 3 3 1 3 2 2 2 3 3 3 3 3 3 2 3 2 3 3 1 2 1 3 3 3 3 2 3 2 3 1\n[334] 3 2 2 1 1 3 2 1 2 1 1 3 1 2 1 3 3 1 2 2 2 3 1 2 3 1 1 2 2 2 2 3 1 3 2 3 3\n[371] 3 3 3 2 2 1 1 3 3 3 3 1 1 3 3 1 1 2 2 1 3 2 2 2 2 1 2 2 1 3 3 3 3 2 2 3 3\n[408] 1 1 1 2 1 1 3 2 2 2 1 2 2 2 3 3 2 3 1 2 2 2 2 1 1 2 1 3 2 3 3 3 1 1 1 1 3\n[445] 2 3 3 3 1 2 3 3 3 2 2 3 2 2 3 1 2 2 3 1 2 2 2 3 1 1 3 1 3 2 2 1 3 1 3 2 2\n[482] 1 3 1 1 1 2 1 1 1 1 1 2 3 3 1 1 3 2 2 2 3 3 3 2 2 1 1 3 1 3 2 2 1 1 3 1 1\n[519] 2 3 3 3 2 1 3 2 1 1 3 2 1 1 1 1 3 3 2 3 3 2 2 2 2 1 3 3 1 3 3 3 2 2 3 2 1\n[556] 1 2 1 1 1 2 1 1 2 1 1 2 1 1 3 1 2 1 2 1 2 2 2 3 1 1 1 1 3 2 1 1 1 2 1 1 2\n[593] 2 2 1 3 2 2 3 2 1 2 2 1 3 3 1 1 1 2 1 3 2 3 1 2 3 2 2 3 1 2 3 1 1 3 2 3 1\n[630] 2 3 1 3 2 2 3 2 3 3 3 3 3 2 2 3 1 3 1 1 3 1 2 1 3 1 3 3 3 2 3 3 2 2 3 3 2\n[667] 3 2 1 1 2 3 2 3 3 1 2 2 3 1 3 2 1 1 3 3 2 3 2 2 1 1 1 1 2 2 2 3 1 2 3 1 1\n[704] 3 3 1 2 1 1 3 2 3 1 2 3 2 2 3 2 2 3 3 2 2 3 2 1 3 3 3 3 3 2 2 3 3 2 2 3 2\n[741] 3 2 1 3 3 3 2 3 1 2 2 3 3 1 3 3 1 3 1 1 3 3 2 1 3 2 1 2 2 3 1 2 1 3 2 3 3\n[778] 1 3 1 2 3 3 2 1 2 3 2 3 1 3 3 2 2 2 3 2 3 1 2 2 2 2 3 3 1 2 3 3 3 3 1 1 3\n[815] 1 3 2 3 2 1 2 2 2 2 2 1 1 2 1 1 1 1 1 1 2 3 3 2 1 1 3 2 1 1 1 1 3 3 2 1 2\n[852] 3 1 1 1 1 3 3 1 1 1 1 1 3 2 1 2 3 2 2 1 3 1 1 3 3 2 3 3 3 1 2 1 2 2 3 3 1\n[889] 1 2 3 1 1 2 2 2 2 3 1 1\n"}],"execution_count":9},{"id":"4944ea46-ed01-413b-8b55-d55f383413de","cell_type":"markdown","source":"#### d) Are your labels/clusters the same? If not, why? Are your means the same?","metadata":{"id":"4944ea46-ed01-413b-8b55-d55f383413de"}},{"id":"AsROp0x_ySbS","cell_type":"markdown","source":"The labels are not the same due to randomization and no set seed. All means for both R's kmeans and my own is -1.03.","metadata":{"id":"AsROp0x_ySbS"}},{"id":"6228e5ce-269f-4d83-b94e-40f3c4a137cd","cell_type":"markdown","source":"## Question 3\n#### a) Explain the process of using a for loop to assign clusters for kmeans.","metadata":{"id":"6228e5ce-269f-4d83-b94e-40f3c4a137cd"}},{"id":"B7WeunNSMccS","cell_type":"markdown","source":"A for loop calculates the distance from every cluster center for each data point and then assigns the closest one to it.","metadata":{"id":"B7WeunNSMccS"}},{"id":"2815b679-8bd1-4ec2-85d3-e42166f3e0ff","cell_type":"markdown","source":"#### b) Explain the process of vectorizing the code to assign clusters for kmeans.","metadata":{"id":"2815b679-8bd1-4ec2-85d3-e42166f3e0ff"}},{"id":"-rofGFBJMzf2","cell_type":"markdown","source":"Instead of running a calculation for each individual data point, vectorizing the code creating arrays that are able to run all distances at once to determine assignments to clusters.","metadata":{"id":"-rofGFBJMzf2"}},{"id":"c6be7b85-c4f0-4626-b7f5-0ba921efcb6d","cell_type":"markdown","source":"#### c) State which (for loops or vectorizing) is more efficient and why.","metadata":{"id":"c6be7b85-c4f0-4626-b7f5-0ba921efcb6d"}},{"id":"X4HNX37fNlVe","cell_type":"markdown","source":"While a slower process to run, vectorizing has the benefit of speed of finding clusters once completed, reaching better performance than for loops when using large datasets, and producing a cleaner line of code compared to the bulkiness of nesting for loops inside each other.","metadata":{"id":"X4HNX37fNlVe"}},{"id":"3d2824d8-b8b6-4746-8fdc-8b3a8a1ca49d","cell_type":"markdown","source":"## Question 4\n#### When does `kmeans` fail? What assumption does `kmeans` use that causes it to fail in this situation?","metadata":{"id":"3d2824d8-b8b6-4746-8fdc-8b3a8a1ca49d"}},{"id":"aXNsS-74DwoQ","cell_type":"markdown","source":"'kmeans' has the potential to fail when one or many of the assumptions are violated. Those assumptions are the following:\n1. The data is in a vector space *(Fails with categorical data or non-linear manifolds)*\n2. The clusters are characterized by their centers or centroids *(Fails when clusters are elongated or concaving shapes)*\n3. Cluster membership falls off at a similar rate from all centroids *(Fails when varience is unequal between clusters)*\n4. Clusters are about the same size *(Fails when clusters are differing sizes or densities)*  \n  \nTogether, kmeans fails when the clusters are not uniform in size and spherical-shape","metadata":{"id":"aXNsS-74DwoQ"}},{"id":"20c5a711-449c-48e0-b02b-8ad3346e8d38","cell_type":"markdown","source":"## Question 5\n#### What assumption do Guassian mixture models make?","metadata":{"id":"20c5a711-449c-48e0-b02b-8ad3346e8d38"}},{"id":"mGW0dXmOGQvx","cell_type":"markdown","source":"Gussian mixture models assume that compared to kmeans, there is a significantly less chance of failure if clusters vary in size and shape by drawing data from N Gaussian distributions.","metadata":{"id":"mGW0dXmOGQvx"}},{"id":"ed1a4f52-f2d5-45d7-aeb2-a05ccab9e2fe","cell_type":"markdown","source":"## Question 6\n#### What assumption does spectral clustering make? Why does this help us?","metadata":{"id":"ed1a4f52-f2d5-45d7-aeb2-a05ccab9e2fe"}},{"id":"AGCV_1ulHOxM","cell_type":"markdown","source":"The closer two points are to one another, the more likely they are to belong to the same cluster. This helps us because this assumption is not restricted by space, size, or any other componet that kmeans could otherwise fail from, it is a simple guide when dealing with complex distributions of data.","metadata":{"id":"AGCV_1ulHOxM"}},{"id":"ede993cf-222f-4c5b-b09e-bc2c988a777e","cell_type":"markdown","source":"## Question 7\n#### Define the gap statistic method. What do we use it for?","metadata":{"id":"ede993cf-222f-4c5b-b09e-bc2c988a777e"}},{"id":"1dcZtFk3I8Ay","cell_type":"markdown","source":"The basic idea is to compare the clustering for each value of K to a cluster of data \"randomized\" into the same domain as the original data. We then compute the dispersion of the two clusterings and look at the difference. We look for a \"knee\" and that is our cluster number. This is ad-hoc but at least standard.\n\nThe gap statistic method determines meanfulness of clusters by comparing our data against randomized data and viewing the difference in result. We use this when determining the number of clusters to assign our data to.","metadata":{"id":"1dcZtFk3I8Ay"}}]}